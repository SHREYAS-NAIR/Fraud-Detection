{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Installing requirements.\")\n",
    "os.system(\"pip3 install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    print(\"Loading data.\")\n",
    "    data = pd.read_csv(\"https://media.githubusercontent.com/media/SHREYASNAIR129/Fraud-Detection/master/Final_Transactions.csv\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def data_Analysis():\n",
    "    data=pd.DataFrame(load_data())\n",
    "    \n",
    "    print(\"Analysing Data\")\n",
    "\n",
    "    #Changing values from scientific notation to much readable notation\n",
    "    print(\"Improving data readability\")\n",
    "    pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "    #First look at the data\n",
    "    print(\"Thease are the first 5 rows of data set.\")\n",
    "    print(data.head())\n",
    "\n",
    "    #Data types\n",
    "    print(\"Observing the data types in the dataset.\")\n",
    "    print(data.info())\n",
    "\n",
    "    #Data set description\n",
    "    print(\"Description of the dataset.\")\n",
    "    print(data.describe())\n",
    "\n",
    "    #Data set null values observation\n",
    "    print(\"Sum of null values in the dataset.\")\n",
    "    print(data.isnull().sum())\n",
    "\n",
    "    #Duplicates in the dataset\n",
    "    print(\"Duplicates in the data are: \",data.duplicated().sum())\n",
    "\n",
    "    #Checking imbalance in data\n",
    "    tx_fraud = data['TX_FRAUD']\n",
    "    print(\"Imbalanced data: \\n\",tx_fraud.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataLoading import load_data\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def data_preprocessing():\n",
    "    #Analysing data before processing it.\n",
    "    data_Analysis()\n",
    "\n",
    "    #Loading data\n",
    "    data = pd.DataFrame(load_data())\n",
    "    \n",
    "    print(\"Preprocessing the data.\")\n",
    "    #Droping null values\n",
    "    print(\"Droping null values\")\n",
    "    data.dropna()\n",
    "    print(\"Null values dropped\")\n",
    "    print(data.isnull().sum())\n",
    "\n",
    "    #Dropping duplicate values\n",
    "    print(\"Droping Duplicate values\")\n",
    "    data.drop_duplicates()\n",
    "    print(\"Duplicates dropped\")\n",
    "\n",
    "    #Random Oversampling of majority class to balance data\n",
    "    print(\"Oversampling to balance data\")\n",
    "    tx_fraud = data['TX_FRAUD']\n",
    "    rus = RandomOverSampler(sampling_strategy=\"minority\")\n",
    "    new_balanced_data, new_tx_fraud = rus.fit_resample(data,tx_fraud)\n",
    "\n",
    "    #Check the new balanced data\n",
    "    print(\"Balanced data: \\n\",new_tx_fraud.value_counts())\n",
    "\n",
    "    return(new_balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def data_Visualization():\n",
    "    data = pd.DataFrame(data_preprocessing())\n",
    "\n",
    "    print(\"Visualizing the data.\")\n",
    "    #To visualize outliers\n",
    "    seaborn.pairplot(data = data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def feature_Engineering():\n",
    "    #Visualizing data\n",
    "    data_Visualization()\n",
    "\n",
    "    data = pd.DataFrame(data_preprocessing())\n",
    "\n",
    "    print(\"Feature Engineering.\")\n",
    "    #Removing outliers and unwanted columns\n",
    "    print(\"Removing outliers and unwanted columns\")\n",
    "    data = data.drop(['Unnamed: 0'], axis=1)\n",
    "    data = data.drop(['TX_DATETIME'], axis=1)\n",
    "    data = data[(data[\"TX_AMOUNT\"]<75000)]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking corelation matrix for essential features.\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def feature_selection():\n",
    "    data = pd.DataFrame(feature_Engineering())\n",
    "    \n",
    "    print(\"Feature selection based on corelation matrix\")\n",
    "    #Checking corelation matrix for essential features.\n",
    "    corr_matrix = data.corr()\n",
    "    sn.heatmap(corr_matrix, annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Droping Columns that have very less corelation\n",
    "    data = data.drop(['TRANSACTION_ID'], axis=1)\n",
    "    data = data.drop(['CUSTOMER_ID'], axis=1)\n",
    "    data = data.drop(['TERMINAL_ID'], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def data_Spliting():\n",
    "    data = pd.DataFrame(feature_selection())\n",
    "\n",
    "    print(\"Splitting the data.\")\n",
    "    # Assigning the featurs as X and trarget as y\n",
    "    X= data.drop([\"TX_FRAUD\"],axis =1)\n",
    "    y= data[\"TX_FRAUD\"]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=7)\n",
    "    return (x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "\n",
    "# Mentioning the models for Classification\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "lr = LogisticRegression()\n",
    "knc = KNeighborsClassifier()\n",
    "svc = LinearSVC()\n",
    "\n",
    "def modelSelection():\n",
    "    x_train, x_test, y_train, y_test = data_Spliting()\n",
    "    \n",
    "    #Name of algorithms used.\n",
    "    algo_names = [\"Decision Tree\",\"Logistic Regression\",\"Random Forest Classification\",\"Gradient Boosting Classification\",\"Linear Support Vector Classification\",\"K Neighbours Classification\"]\n",
    "    algo = [dtc, lr, rfc, gbc, svc, knc] \n",
    "    \n",
    "    print(\"Selecting the best model\")\n",
    "    #Finding AI Scores for each classifier.\n",
    "    Al_Scores = []\n",
    "    j=0\n",
    "    for i in algo:\n",
    "        train = i.fit(x_train,y_train)\n",
    "        score = train.score(x_test,y_test)\n",
    "        Al_Scores.append(score)\n",
    "        # print(score)\n",
    "        print(algo_names[j],\":\",score*100)\n",
    "        j+=1\n",
    "    \n",
    "    #Deciding final model\n",
    "    max_value= max(Al_Scores)\n",
    "    for i in range(len(Al_Scores)):\n",
    "        if Al_Scores[i] == max_value:\n",
    "            best_model = algo[i]\n",
    "            best_model_name = algo_names[i]\n",
    "    print(\"The best model is\", best_model_name)\n",
    "\n",
    "    #Finding metrics for the best model and printing them\n",
    "    y_predict = best_model.predict(x_test)\n",
    "    precision = precision_score(y_test, y_predict, pos_label='positive', average='micro')\n",
    "    recall = recall_score(y_test, y_predict, pos_label='positive', average='micro')\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    f1score = f1_score(y_test, y_predict, pos_label='positive', average='micro')\n",
    "\n",
    "    print(\"Precision = \", precision)\n",
    "    print(\"f1score = \", f1score)\n",
    "    print(\"recall = \", recall)\n",
    "    print(\"accuracy = \", accuracy)\n",
    "\n",
    "    #Creating pickle file for best model\n",
    "    print(\"Creating pickel file.\")\n",
    "    pickle.dump(best_model, open('model.pkl', 'wb'))\n",
    "    \n",
    "modelSelection()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b3c739073ce9d82a91414a5eee8662c2dabaa65c7088790b517cd876b892603"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
